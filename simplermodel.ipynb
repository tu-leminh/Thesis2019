{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_category</th>\n",
       "      <th>channel_started</th>\n",
       "      <th>channel_rank</th>\n",
       "      <th>channel_subscribers</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>title_length</th>\n",
       "      <th>categories</th>\n",
       "      <th>...</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>like_per_view</th>\n",
       "      <th>comment_per_view</th>\n",
       "      <th>dislike_per_view</th>\n",
       "      <th>engagement_rate_1</th>\n",
       "      <th>engagement_rate_2</th>\n",
       "      <th>q_score</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC0jDoh3tVXCaqJ6oTve8ebA</td>\n",
       "      <td>FAP TV</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>12800000</td>\n",
       "      <td>nehrVdADdH0</td>\n",
       "      <td>[FAP TV ] Thông Báo Tuyển Diễn Viên Nam Film L...</td>\n",
       "      <td>14</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>...</td>\n",
       "      <td>827</td>\n",
       "      <td>105</td>\n",
       "      <td>0.020731</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.970364</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UC0jDoh3tVXCaqJ6oTve8ebA</td>\n",
       "      <td>FAP TV</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>12800000</td>\n",
       "      <td>K66wOEaBwK4</td>\n",
       "      <td>Phía Sau Một Cô Gái - Soobin Hoàng Sơn | MV Fa...</td>\n",
       "      <td>14</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>...</td>\n",
       "      <td>1594</td>\n",
       "      <td>664</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>0.950070</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UC0jDoh3tVXCaqJ6oTve8ebA</td>\n",
       "      <td>FAP TV</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>12800000</td>\n",
       "      <td>D00vn3X7oI8</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 94 - Dấu Ấn Học Đường Phần 2</td>\n",
       "      <td>12</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>...</td>\n",
       "      <td>2214</td>\n",
       "      <td>3089</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>0.896920</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UC0jDoh3tVXCaqJ6oTve8ebA</td>\n",
       "      <td>FAP TV</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>12800000</td>\n",
       "      <td>G22G1k3G-kM</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 100 - Hành Trình Vui Vẻ</td>\n",
       "      <td>10</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>...</td>\n",
       "      <td>1752</td>\n",
       "      <td>2202</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.008159</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>0.930209</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UC0jDoh3tVXCaqJ6oTve8ebA</td>\n",
       "      <td>FAP TV</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>12800000</td>\n",
       "      <td>G5EG7ymPErw</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 95 - Dấu Ấn Học Đường Phầ...</td>\n",
       "      <td>12</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>...</td>\n",
       "      <td>2417</td>\n",
       "      <td>2208</td>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.918029</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id channel_name channel_category  channel_started  \\\n",
       "0  UC0jDoh3tVXCaqJ6oTve8ebA       FAP TV           Comedy             2014   \n",
       "1  UC0jDoh3tVXCaqJ6oTve8ebA       FAP TV           Comedy             2014   \n",
       "2  UC0jDoh3tVXCaqJ6oTve8ebA       FAP TV           Comedy             2014   \n",
       "3  UC0jDoh3tVXCaqJ6oTve8ebA       FAP TV           Comedy             2014   \n",
       "4  UC0jDoh3tVXCaqJ6oTve8ebA       FAP TV           Comedy             2014   \n",
       "\n",
       "   channel_rank  channel_subscribers           id  \\\n",
       "0             2             12800000  nehrVdADdH0   \n",
       "1             2             12800000  K66wOEaBwK4   \n",
       "2             2             12800000  D00vn3X7oI8   \n",
       "3             2             12800000  G22G1k3G-kM   \n",
       "4             2             12800000  G5EG7ymPErw   \n",
       "\n",
       "                                               title  title_length  \\\n",
       "0  [FAP TV ] Thông Báo Tuyển Diễn Viên Nam Film L...            14   \n",
       "1  Phía Sau Một Cô Gái - Soobin Hoàng Sơn | MV Fa...            14   \n",
       "2  FAPtv Cơm Nguội: Tập 94 - Dấu Ấn Học Đường Phần 2            12   \n",
       "3       FAPtv Cơm Nguội: Tập 100 - Hành Trình Vui Vẻ            10   \n",
       "4  FAPtv Cơm Nguội: Tập 95 - Dấu Ấn Học Đường Phầ...            12   \n",
       "\n",
       "         categories  ... comment_count dislike_count  like_per_view  \\\n",
       "0  Film & Animation  ...           827           105       0.020731   \n",
       "1            Comedy  ...          1594           664       0.010124   \n",
       "2     Entertainment  ...          2214          3089       0.006007   \n",
       "3     Entertainment  ...          1752          2202       0.007931   \n",
       "4     Entertainment  ...          2417          2208       0.006485   \n",
       "\n",
       "  comment_per_view  dislike_per_view  engagement_rate_1  engagement_rate_2  \\\n",
       "0         0.002456          0.000312           0.023187           0.023499   \n",
       "1         0.000622          0.000259           0.010746           0.011005   \n",
       "2         0.000234          0.000326           0.006241           0.006568   \n",
       "3         0.000228          0.000287           0.008159           0.008446   \n",
       "4         0.000303          0.000277           0.006789           0.007066   \n",
       "\n",
       "    q_score  label_1  label_2  \n",
       "0  0.970364        2        2  \n",
       "1  0.950070        1        2  \n",
       "2  0.896920        1        2  \n",
       "3  0.930209        1        2  \n",
       "4  0.918029        1        2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertConfig\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "print(phobert)\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "\n",
    "\n",
    "ENTUBE = 'entube.parquet'\n",
    "entube = pd.read_parquet(ENTUBE)\n",
    "entube.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(text,s=100):\n",
    "    temp=tokenizer.encode(text)\n",
    "    while len(temp)<s:\n",
    "        temp.append(1)\n",
    "    input_ids = torch.tensor(temp[:s])\n",
    "    return input_ids\n",
    "\n",
    "def get_tags_encode(tags_list):\n",
    "  return get_encode(\" \".join([i for i in tags_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nehrVdADdH0</td>\n",
       "      <td>[FAP TV ] Thông Báo Tuyển Diễn Viên Nam Film L...</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K66wOEaBwK4</td>\n",
       "      <td>Phía Sau Một Cô Gái - Soobin Hoàng Sơn | MV Fa...</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D00vn3X7oI8</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 94 - Dấu Ấn Học Đường Phần 2</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G22G1k3G-kM</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 100 - Hành Trình Vui Vẻ</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G5EG7ymPErw</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 95 - Dấu Ấn Học Đường Phầ...</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26231</th>\n",
       "      <td>3Dn6gHcMMxw</td>\n",
       "      <td>Hot: Cha Đẻ \"ATM gạo\" Lần Đầu Ra Mắt \"ATM khẩu...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26232</th>\n",
       "      <td>GhxY-7QwD_c</td>\n",
       "      <td>Đường Thành Sông, Nhà Thành Hầm Chứa Nước Sau ...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26233</th>\n",
       "      <td>MwqkdX9lyuk</td>\n",
       "      <td>AEON MALL Bình Tân Quá Th.ê Thảm, Vắ.ng Hoe Vì...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26234</th>\n",
       "      <td>3dHgXfekVjo</td>\n",
       "      <td>Sài Gòn Ngập Khắp Nơi Từ Chiều Đến Đêm, Ngang ...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26235</th>\n",
       "      <td>tpXTz_0FjEA</td>\n",
       "      <td>Nước Chảy Cuồn Cuộn Như Thác Lũ Trên Phố Sài G...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26236 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                              title  \\\n",
       "0      nehrVdADdH0  [FAP TV ] Thông Báo Tuyển Diễn Viên Nam Film L...   \n",
       "1      K66wOEaBwK4  Phía Sau Một Cô Gái - Soobin Hoàng Sơn | MV Fa...   \n",
       "2      D00vn3X7oI8  FAPtv Cơm Nguội: Tập 94 - Dấu Ấn Học Đường Phần 2   \n",
       "3      G22G1k3G-kM       FAPtv Cơm Nguội: Tập 100 - Hành Trình Vui Vẻ   \n",
       "4      G5EG7ymPErw  FAPtv Cơm Nguội: Tập 95 - Dấu Ấn Học Đường Phầ...   \n",
       "...            ...                                                ...   \n",
       "26231  3Dn6gHcMMxw  Hot: Cha Đẻ \"ATM gạo\" Lần Đầu Ra Mắt \"ATM khẩu...   \n",
       "26232  GhxY-7QwD_c  Đường Thành Sông, Nhà Thành Hầm Chứa Nước Sau ...   \n",
       "26233  MwqkdX9lyuk  AEON MALL Bình Tân Quá Th.ê Thảm, Vắ.ng Hoe Vì...   \n",
       "26234  3dHgXfekVjo  Sài Gòn Ngập Khắp Nơi Từ Chiều Đến Đêm, Ngang ...   \n",
       "26235  tpXTz_0FjEA  Nước Chảy Cuồn Cuộn Như Thác Lũ Trên Phố Sài G...   \n",
       "\n",
       "                                                    tags  label_2  \n",
       "0      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2  \n",
       "1      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2  \n",
       "2      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2  \n",
       "3      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2  \n",
       "4      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2  \n",
       "...                                                  ...      ...  \n",
       "26231    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1  \n",
       "26232    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1  \n",
       "26233    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1  \n",
       "26234    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1  \n",
       "26235    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1  \n",
       "\n",
       "[26236 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entube_data = entube[['id','title','tags','label_2']]\n",
    "entube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtthe\\AppData\\Local\\Temp\\ipykernel_19260\\3004365453.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  entube_data['encode_title'] = entube_data['title'].apply(get_encode)\n",
      "C:\\Users\\mtthe\\AppData\\Local\\Temp\\ipykernel_19260\\3004365453.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  entube_data['encode_tag'] = entube_data['tags'].apply(get_tags_encode)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>label_2</th>\n",
       "      <th>encode_title</th>\n",
       "      <th>encode_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nehrVdADdH0</td>\n",
       "      <td>[FAP TV ] Thông Báo Tuyển Diễn Viên Nam Film L...</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "      <td>[tensor(0), tensor(63576), tensor(1579), tenso...</td>\n",
       "      <td>[tensor(0), tensor(266), tensor(82), tensor(15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K66wOEaBwK4</td>\n",
       "      <td>Phía Sau Một Cô Gái - Soobin Hoàng Sơn | MV Fa...</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "      <td>[tensor(0), tensor(2696), tensor(162), tensor(...</td>\n",
       "      <td>[tensor(0), tensor(266), tensor(82), tensor(15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D00vn3X7oI8</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 94 - Dấu Ấn Học Đường Phần 2</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "      <td>[tensor(0), tensor(1579), tensor(8607), tensor...</td>\n",
       "      <td>[tensor(0), tensor(266), tensor(82), tensor(15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G22G1k3G-kM</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 100 - Hành Trình Vui Vẻ</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "      <td>[tensor(0), tensor(1579), tensor(8607), tensor...</td>\n",
       "      <td>[tensor(0), tensor(266), tensor(82), tensor(15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G5EG7ymPErw</td>\n",
       "      <td>FAPtv Cơm Nguội: Tập 95 - Dấu Ấn Học Đường Phầ...</td>\n",
       "      <td>[phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...</td>\n",
       "      <td>2</td>\n",
       "      <td>[tensor(0), tensor(1579), tensor(8607), tensor...</td>\n",
       "      <td>[tensor(0), tensor(266), tensor(82), tensor(15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26231</th>\n",
       "      <td>3Dn6gHcMMxw</td>\n",
       "      <td>Hot: Cha Đẻ \"ATM gạo\" Lần Đầu Ra Mắt \"ATM khẩu...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tensor(0), tensor(16261), tensor(27), tensor(...</td>\n",
       "      <td>[tensor(0), tensor(350), tensor(590), tensor(4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26232</th>\n",
       "      <td>GhxY-7QwD_c</td>\n",
       "      <td>Đường Thành Sông, Nhà Thành Hầm Chứa Nước Sau ...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tensor(0), tensor(2080), tensor(1206), tensor...</td>\n",
       "      <td>[tensor(0), tensor(350), tensor(590), tensor(4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26233</th>\n",
       "      <td>MwqkdX9lyuk</td>\n",
       "      <td>AEON MALL Bình Tân Quá Th.ê Thảm, Vắ.ng Hoe Vì...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tensor(0), tensor(40989), tensor(11778), tens...</td>\n",
       "      <td>[tensor(0), tensor(350), tensor(590), tensor(4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26234</th>\n",
       "      <td>3dHgXfekVjo</td>\n",
       "      <td>Sài Gòn Ngập Khắp Nơi Từ Chiều Đến Đêm, Ngang ...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tensor(0), tensor(10696), tensor(26328), tens...</td>\n",
       "      <td>[tensor(0), tensor(350), tensor(590), tensor(4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26235</th>\n",
       "      <td>tpXTz_0FjEA</td>\n",
       "      <td>Nước Chảy Cuồn Cuộn Như Thác Lũ Trên Phố Sài G...</td>\n",
       "      <td>[Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]</td>\n",
       "      <td>1</td>\n",
       "      <td>[tensor(0), tensor(2837), tensor(33020), tenso...</td>\n",
       "      <td>[tensor(0), tensor(350), tensor(590), tensor(4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26236 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                              title  \\\n",
       "0      nehrVdADdH0  [FAP TV ] Thông Báo Tuyển Diễn Viên Nam Film L...   \n",
       "1      K66wOEaBwK4  Phía Sau Một Cô Gái - Soobin Hoàng Sơn | MV Fa...   \n",
       "2      D00vn3X7oI8  FAPtv Cơm Nguội: Tập 94 - Dấu Ấn Học Đường Phần 2   \n",
       "3      G22G1k3G-kM       FAPtv Cơm Nguội: Tập 100 - Hành Trình Vui Vẻ   \n",
       "4      G5EG7ymPErw  FAPtv Cơm Nguội: Tập 95 - Dấu Ấn Học Đường Phầ...   \n",
       "...            ...                                                ...   \n",
       "26231  3Dn6gHcMMxw  Hot: Cha Đẻ \"ATM gạo\" Lần Đầu Ra Mắt \"ATM khẩu...   \n",
       "26232  GhxY-7QwD_c  Đường Thành Sông, Nhà Thành Hầm Chứa Nước Sau ...   \n",
       "26233  MwqkdX9lyuk  AEON MALL Bình Tân Quá Th.ê Thảm, Vắ.ng Hoe Vì...   \n",
       "26234  3dHgXfekVjo  Sài Gòn Ngập Khắp Nơi Từ Chiều Đến Đêm, Ngang ...   \n",
       "26235  tpXTz_0FjEA  Nước Chảy Cuồn Cuộn Như Thác Lũ Trên Phố Sài G...   \n",
       "\n",
       "                                                    tags  label_2  \\\n",
       "0      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2   \n",
       "1      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2   \n",
       "2      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2   \n",
       "3      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2   \n",
       "4      [phim hai, FAPtv, FAPtivi, cơm nguội, Ribi Sac...        2   \n",
       "...                                                  ...      ...   \n",
       "26231    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1   \n",
       "26232    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1   \n",
       "26233    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1   \n",
       "26234    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1   \n",
       "26235    [Việt Nam Fly, Việt Nam, Fly, Việt Nam bay cao]        1   \n",
       "\n",
       "                                            encode_title  \\\n",
       "0      [tensor(0), tensor(63576), tensor(1579), tenso...   \n",
       "1      [tensor(0), tensor(2696), tensor(162), tensor(...   \n",
       "2      [tensor(0), tensor(1579), tensor(8607), tensor...   \n",
       "3      [tensor(0), tensor(1579), tensor(8607), tensor...   \n",
       "4      [tensor(0), tensor(1579), tensor(8607), tensor...   \n",
       "...                                                  ...   \n",
       "26231  [tensor(0), tensor(16261), tensor(27), tensor(...   \n",
       "26232  [tensor(0), tensor(2080), tensor(1206), tensor...   \n",
       "26233  [tensor(0), tensor(40989), tensor(11778), tens...   \n",
       "26234  [tensor(0), tensor(10696), tensor(26328), tens...   \n",
       "26235  [tensor(0), tensor(2837), tensor(33020), tenso...   \n",
       "\n",
       "                                              encode_tag  \n",
       "0      [tensor(0), tensor(266), tensor(82), tensor(15...  \n",
       "1      [tensor(0), tensor(266), tensor(82), tensor(15...  \n",
       "2      [tensor(0), tensor(266), tensor(82), tensor(15...  \n",
       "3      [tensor(0), tensor(266), tensor(82), tensor(15...  \n",
       "4      [tensor(0), tensor(266), tensor(82), tensor(15...  \n",
       "...                                                  ...  \n",
       "26231  [tensor(0), tensor(350), tensor(590), tensor(4...  \n",
       "26232  [tensor(0), tensor(350), tensor(590), tensor(4...  \n",
       "26233  [tensor(0), tensor(350), tensor(590), tensor(4...  \n",
       "26234  [tensor(0), tensor(350), tensor(590), tensor(4...  \n",
       "26235  [tensor(0), tensor(350), tensor(590), tensor(4...  \n",
       "\n",
       "[26236 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entube_data['encode_title'] = entube_data['title'].apply(get_encode)\n",
    "entube_data['encode_tag'] = entube_data['tags'].apply(get_tags_encode)\n",
    "entube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "class SimplerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplerModel, self).__init__()\n",
    "\n",
    "        self.phobert_title = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.title_LayerNorm = nn.LayerNorm(768)\n",
    "        self.title_MultiheadAttention = nn.MultiheadAttention(768,1)\n",
    "        self.title_Linear = nn.Linear(768, 1024)\n",
    "\n",
    "\n",
    "        self.phobert_tag = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.tag_LayerNorm = nn.LayerNorm(768)\n",
    "        self.tag_MultiheadAttention = nn.MultiheadAttention(768,1)\n",
    "        self.tag_Linear = nn.Linear(768, 1024)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.BatchNorm1d(2),\n",
    "                nn.Conv1d(2, 1, 1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.cf = nn.Linear(1024,3)\n",
    "    def forward(self, X):\n",
    "        if (len(X.shape)==1):\n",
    "            X = X[None,:]\n",
    "        X_title = X[:,:100]\n",
    "        X_tag = X[:,100:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_title = self.phobert_title(X_title)\n",
    "        x_title = x_title['pooler_output']\n",
    "        x_title_1 = x_title\n",
    "        x_title = self.title_LayerNorm(x_title)\n",
    "        x_title,_ = self.title_MultiheadAttention(x_title,x_title,x_title)\n",
    "        x_title=x_title+x_title_1\n",
    "        x_title = self.title_Linear(x_title)\n",
    "        x_title = x_title[:,None,:]\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x_tag = self.phobert_tag(X_tag)\n",
    "        x_tag = x_tag['pooler_output']\n",
    "        x_tag_1 = x_tag\n",
    "        x_tag = self.tag_LayerNorm(x_tag)\n",
    "        x_tag,_ = self.tag_MultiheadAttention(x_tag,x_tag,x_tag)\n",
    "        x_tag=x_tag+x_tag_1\n",
    "        x_tag = self.tag_Linear(x_tag)\n",
    "        x_tag = x_tag[:,None,:]\n",
    "\n",
    "        \n",
    "        x = torch.concat((x_title, x_tag), dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.cf(x)\n",
    "        return torch.squeeze(x)\n",
    "simplermodel = SimplerModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} device')\n",
    "simplermodel = simplermodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index].to_numpy()\n",
    "        features =  torch.cat((row[4],row[5]),0)\n",
    "        label = row[3]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "data = CustomDataset(dataframe=entube_data)\n",
    "dataloader = DataLoader(data, batch_size = 20, drop_last=True,shuffle=True)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "generator = torch.Generator().manual_seed(123)\n",
    "train_loader, val_loader = torch.utils.data.random_split(data, [train_size, test_size],generator=generator)\n",
    "trainloader = DataLoader(train_loader, batch_size=32)\n",
    "validloader = DataLoader(val_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(simplermodel.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"saved_model.pth\"\n",
    "simplermodel.load_state_dict(torch.load(PATH))\n",
    "\n",
    "def valid(e,train_loss,min_valid_loss = np.inf):\n",
    "    valid_loss = 0.0\n",
    "    simplermodel.eval()\n",
    "    for data, labels in validloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        target = simplermodel(data)\n",
    "        loss = criterion(target,labels)\n",
    "        valid_loss+= loss.item()\n",
    "\n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(trainloader)} \\t\\t Validation Loss: {valid_loss / len(validloader)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(simplermodel.state_dict(), PATH)\n",
    "    return min_valid_loss\n",
    "def train():\n",
    "    train_loss = 0.0\n",
    "    simplermodel.train()\n",
    "    for data, labels in tqdm(trainloader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        target = simplermodel(data)\n",
    "        loss = criterion(target,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.0 \t\t Validation Loss: 0.7885131308945214\n",
      "Validation Loss Decreased(inf--->129.316153) \t Saving The Model\n"
     ]
    }
   ],
   "source": [
    "epochs=0\n",
    "\n",
    "min_valid_loss=valid(0,0)\n",
    "for e in range(epochs):\n",
    "    train_loss=train()\n",
    "    min_valid_loss=valid(e,train_loss,min_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimplerModel(\n",
       "  (phobert_title): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (title_LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (title_MultiheadAttention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (title_Linear): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (phobert_tag): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (tag_LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (tag_MultiheadAttention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (tag_Linear): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (conv): Sequential(\n",
       "    (0): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv1d(2, 1, kernel_size=(1,), stride=(1,))\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (cf): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplermodel.phobert_title.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(\"Xuân Hinh Thanh Ngoan | Vợ Chồng Quán | Xuân Hinh Hat Chèo Hay Nhất\", ref_token_id, sep_token_id, cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(custom_forward_0, simplermodel.phobert_title.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attributions, delta \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39;49mattribute(inputs\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m      2\u001b[0m                                     baselines\u001b[39m=\u001b[39;49mref_input_ids,\n\u001b[0;32m      3\u001b[0m                                     n_steps\u001b[39m=\u001b[39;49m\u001b[39m700\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m                                     internal_batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m                                     return_convergence_delta\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\captum\\attr\\_core\\layer\\layer_integrated_gradients.py:371\u001b[0m, in \u001b[0;36mLayerIntegratedGradients.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_func, \u001b[39m\"\u001b[39m\u001b[39mdevice_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 371\u001b[0m inputs_layer \u001b[39m=\u001b[39m _forward_layer_eval(\n\u001b[0;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[0;32m    373\u001b[0m     inps,\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer,\n\u001b[0;32m    375\u001b[0m     device_ids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids,\n\u001b[0;32m    376\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[0;32m    377\u001b[0m     attribute_to_layer_input\u001b[39m=\u001b[39;49mattribute_to_layer_input,\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m \u001b[39m# if we have one output\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\captum\\_utils\\gradient.py:182\u001b[0m, in \u001b[0;36m_forward_layer_eval\u001b[1;34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_layer_eval\u001b[39m(\n\u001b[0;32m    174\u001b[0m     forward_fn: Callable,\n\u001b[0;32m    175\u001b[0m     inputs: Union[Tensor, Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m     grad_enabled: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    181\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], List[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m _forward_layer_eval_with_neuron_grads(\n\u001b[0;32m    183\u001b[0m         forward_fn,\n\u001b[0;32m    184\u001b[0m         inputs,\n\u001b[0;32m    185\u001b[0m         layer,\n\u001b[0;32m    186\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[0;32m    187\u001b[0m         gradient_neuron_selector\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    188\u001b[0m         grad_enabled\u001b[39m=\u001b[39;49mgrad_enabled,\n\u001b[0;32m    189\u001b[0m         device_ids\u001b[39m=\u001b[39;49mdevice_ids,\n\u001b[0;32m    190\u001b[0m         attribute_to_layer_input\u001b[39m=\u001b[39;49mattribute_to_layer_input,\n\u001b[0;32m    191\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\captum\\_utils\\gradient.py:445\u001b[0m, in \u001b[0;36m_forward_layer_eval_with_neuron_grads\u001b[1;34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    442\u001b[0m grad_enabled \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m gradient_neuron_selector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m grad_enabled\n\u001b[0;32m    444\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_grad_enabled(grad_enabled):\n\u001b[1;32m--> 445\u001b[0m     saved_layer \u001b[39m=\u001b[39m _forward_layer_distributed_eval(\n\u001b[0;32m    446\u001b[0m         forward_fn,\n\u001b[0;32m    447\u001b[0m         inputs,\n\u001b[0;32m    448\u001b[0m         layer,\n\u001b[0;32m    449\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[0;32m    450\u001b[0m         attribute_to_layer_input\u001b[39m=\u001b[39;49mattribute_to_layer_input,\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m device_ids \u001b[39m=\u001b[39m _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001b[0;32m    453\u001b[0m \u001b[39m# Identifies correct device ordering based on device ids.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# key_list is a list of devices in appropriate ordering for concatenation.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# If only one key exists (standard model), key list simply has one element.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\captum\\_utils\\gradient.py:294\u001b[0m, in \u001b[0;36m_forward_layer_distributed_eval\u001b[1;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m             all_hooks\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    292\u001b[0m                 single_layer\u001b[39m.\u001b[39mregister_forward_hook(hook_wrapper(single_layer))\n\u001b[0;32m    293\u001b[0m             )\n\u001b[1;32m--> 294\u001b[0m     output \u001b[39m=\u001b[39m _run_forward(\n\u001b[0;32m    295\u001b[0m         forward_fn,\n\u001b[0;32m    296\u001b[0m         inputs,\n\u001b[0;32m    297\u001b[0m         target\u001b[39m=\u001b[39;49mtarget_ind,\n\u001b[0;32m    298\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[0;32m    299\u001b[0m     )\n\u001b[0;32m    300\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m all_hooks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\captum\\_utils\\common.py:482\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    479\u001b[0m inputs \u001b[39m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    480\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 482\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[0;32m    483\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49madditional_forward_args)\n\u001b[0;32m    484\u001b[0m     \u001b[39mif\u001b[39;49;00m additional_forward_args \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    485\u001b[0m     \u001b[39melse\u001b[39;49;00m inputs\n\u001b[0;32m    486\u001b[0m )\n\u001b[0;32m    487\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m, in \u001b[0;36mcustom_forward_0\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_forward_0\u001b[39m(inputs):\n\u001b[1;32m----> 5\u001b[0m     preds \u001b[39m=\u001b[39m predict(inputs)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msoftmax(preds, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)[:, \u001b[39m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(inputs):\n\u001b[1;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m simplermodel(inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m, in \u001b[0;36mSimplerModel.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     36\u001b[0m x_title \u001b[39m=\u001b[39m x_title[:,\u001b[39mNone\u001b[39;00m,:]\n\u001b[0;32m     39\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 40\u001b[0m     x_tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphobert_tag(X_tag)\n\u001b[0;32m     41\u001b[0m x_tag \u001b[39m=\u001b[39m x_tag[\u001b[39m'\u001b[39m\u001b[39mpooler_output\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m x_tag_1 \u001b[39m=\u001b[39m x_tag\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:865\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m    868\u001b[0m     \u001b[39mreturn\u001b[39;00m (sequence_output, pooled_output) \u001b[39m+\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:579\u001b[0m, in \u001b[0;36mRobertaPooler.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    577\u001b[0m     \u001b[39m# We \"pool\" the model by simply taking the hidden state corresponding\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[39m# to the first token.\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m     first_token_tensor \u001b[39m=\u001b[39m hidden_states[:, \u001b[39m0\u001b[39;49m]\n\u001b[0;32m    580\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(first_token_tensor)\n\u001b[0;32m    581\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(pooled_output)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                    baselines=ref_input_ids,\n",
    "                                    n_steps=700,\n",
    "                                    internal_batch_size=32,\n",
    "                                    return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    return simplermodel(inputs)\n",
    "\n",
    "def custom_forward_0(inputs):\n",
    "    preds = predict(inputs)\n",
    "    return torch.softmax(preds, dim = 1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for data, labels in validloader:\n",
    "    if torch.cuda.is_available():\n",
    "        data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    target = simplermodel(data)\n",
    "    loss = criterion(target,labels)\n",
    "    print(custom_forward_0(data).shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
